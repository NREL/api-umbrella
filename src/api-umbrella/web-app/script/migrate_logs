#!/usr/bin/env ruby

# A script to migrate analytics log data from a separate ElasticSearch cluster
# to a new one (where it can be combined with other new data).
#
# This is for the migration of developer.nrel.gov's older and separate API
# Umbrella installation onto api.data.gov's newer one.
#
# Setup compressed double tunnel to directly expose ElasticSearch on DB server
# (via NAT server):
# ssh -C -L 9999:$DB_INTERNAL_IP:9200 -N $NAT_IP
#
# Run this script locally to copy data from $CURRENT_SERVER into remote
# destination DB server:
# SOURCE="http://$CURRENT_SERVER:9200" DEST="http://127.0.0.1:9999" EXPECTED_HOST="example.com" bundle exec ./script/migrate_logs

require "elasticsearch"
require "logger"

STDOUT.sync = true
logger = Logger.new(STDOUT)

source_client = Elasticsearch::Client.new(:url => ENV["SOURCE"])
dest_client = Elasticsearch::Client.new(:url => ENV["DEST"])

# Manually sync the geocoding index by explicitly merging to pick the most
# recent result for a location.
source_index = "api-umbrella"
dest_index = "api-umbrella"
logger.info "#{source_index} => #{dest_index}"
source_geocodes = source_client.search(:index => source_index, :size => 100_000)["hits"]["hits"]
existing_geocodes = dest_client.search(:index => dest_index, :size => 100_000)["hits"]["hits"]
bulk_commands = []
source_geocodes.each do |source_geocode|
  existing_geocode = existing_geocodes.detect { |dg| dg["_id"] == source_geocode["_id"] }
  if(existing_geocode && existing_geocode["_source"]["updated_at"] >= source_geocode["_source"]["updated_at"])
    next
  end

  bulk_commands << { :index => { :_index => dest_index, :_type => source_geocode["_type"], :_id => source_geocode["_id"] } }
  bulk_commands << source_geocode["_source"]
end

if(bulk_commands.any?)
  dest_client.bulk(:body => bulk_commands)
end

indices = source_client.indices.get_aliases.keys.sort
indices.each do |source_index| # rubocop:disable Lint/ShadowingOuterLocalVariable
  dest_index = source_index.dup
  if(source_index =~ /^api-umbrella-logs-v1-production-\d\d\d\d-\d\d$/)
    dest_index.gsub!(/-production-/, "-")
  elsif(source_index == "api-umbrella")
    # The geocoding collection - Migrated manually above to pick out most
    # recent from duplicates.
    next
  else
    logger.error "Unknown index: #{source_index}"
    next
  end

  logger.info "#{source_index} => #{dest_index}"

  if(ENV["SKIP_LESS_THAN_INDEX"] && dest_index < ENV["SKIP_LESS_THAN_INDEX"])
    logger.info "Skipping #{dest_index} index"
    next
  end

  result = source_client.search(:index => source_index, :search_type => "scan", :scroll => "10m", :size => 5000)
  scroll_id = result["_scroll_id"]
  total_hits = result["hits"]["total"]
  count = 0
  while(scroll = source_client.scroll(:scroll_id => scroll_id, :scroll => "10m")) # rubocop:disable Lint/AssignmentInCondition
    start_time = Time.now.utc

    scroll_id = scroll["_scroll_id"]
    hits = scroll["hits"]["hits"]

    # Break when elasticsearch returns empty hits (we've reached the end).
    break if hits.empty?

    bulk_commands = []
    hits.each do |hit|
      count += 1

      # Ensure the record being imported has the expected host (just to double
      # check that the imported records can be clearly distinguished).
      if(!hit["_source"]["request_host"] || !hit["_source"]["request_host"].include?(ENV["EXPECTED_HOST"]))
        logger.error "Record did not contain expected request_host field. Skipping. #{hit["_id"].inspect}"
        next
      end

      # Translate to the new way of storing hierarchy path information.
      hit["_source"].delete("request_path_hierarchy")
      if(!hit["_source"]["request_hierarchy"])
        hierarchy_string = "#{hit["_source"]["request_host"]}#{hit["_source"]["request_path"]}"
        hierarchy_string.gsub!(%r{//+}, "/")
        hierarchy_string.chomp!("/")
        hierarchy = []
        hierarchy_parts = hierarchy_string.split("/")
        (0...hierarchy_parts.length).each do |index|
          parents_and_self = hierarchy_parts[0..index]
          token = "#{index}/#{parents_and_self.join("/")}"

          if(index < hierarchy_parts.length - 1)
            token << "/"
          end

          hierarchy << token
        end
        hit["_source"]["request_hierarchy"] = hierarchy
      end

      # Mark this record as imported, so we can distinguish the imported hits.
      hit["_source"]["imported"] = true

      bulk_commands << { :index => { :_index => dest_index, :_type => hit["_type"], :_id => hit["_id"] } }
      bulk_commands << hit["_source"]
    end

    if(bulk_commands.any?)
      dest_client.bulk(:body => bulk_commands)
      elapsed_time = Time.now.utc - start_time
      logger.info "Indexed #{count} of #{total_hits} (#{bulk_commands.length / 2} records indexed in #{elapsed_time.round} seconds)"
    end
  end

  if(source_index != "api-umbrella")
    # Create the version-less index alias names.
    alias_name = dest_index.gsub("-v1-", "-")
    alias_write_name = dest_index.gsub("-v1-", "-write-")
    indices = dest_client.indices.get_aliases
    if(indices[dest_index] && (!indices[dest_index]["aliases"] || !indices[dest_index]["aliases"][alias_name]))
      dest_client.indices.put_alias(:index => dest_index, :name => alias_name)
    end
    if(indices[dest_index] && (!indices[dest_index]["aliases"] || !indices[dest_index]["aliases"][alias_write_name]))
      dest_client.indices.put_alias(:index => dest_index, :name => alias_write_name)
    end
  end
end
